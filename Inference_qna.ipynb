{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de208818-20fd-473b-8798-651956df097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple inference test for T5 Fine Tuning (Question and Answering) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb66ee5c-f1e0-468c-bfcf-e61c777ab150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/HabanaAI/DeepSpeed.git@1.10.0\n",
      "  Cloning https://github.com/HabanaAI/DeepSpeed.git (to revision 1.10.0) to /tmp/pip-req-build-sfqta8qz\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/HabanaAI/DeepSpeed.git /tmp/pip-req-build-sfqta8qz\n",
      "  Running command git checkout -b 1.10.0 --track origin/1.10.0\n",
      "  Switched to a new branch '1.10.0'\n",
      "  Branch '1.10.0' set up to track remote branch '1.10.0' from 'origin'.\n",
      "  Resolved https://github.com/HabanaAI/DeepSpeed.git to commit 4bc77a676c8c0def07ada747d17b0b7f881bbc8c\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting hjson (from deepspeed==0.7.7+hpu.synapse.v1.10.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.10.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044493bd-f55a-422f-b7e0-b1f80fa8adb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install optimum[habana]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a86d714-fe49-4f42-b4e3-a7182116dc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/shared/demo/optimum-habana/examples/seq2seq_squad'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de772de3-a7b9-432e-9583-c25a9e1fe52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: './seq2seq_squad'\n",
      "/root/shared/demo/optimum-habana/examples/seq2seq_squad\n"
     ]
    }
   ],
   "source": [
    "%cd ./seq2seq_squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d27d821d-4951-48ab-8a32-23e96cbc36e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 432392\n",
      "drwxr-xr-x  6 root root      4096 Aug  1 02:34 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
      "drwxr-xr-x 25 root root      4096 Aug  1 07:39 \u001b[01;34m..\u001b[0m/\n",
      "-rw-r--r--  1 root root       958 Aug  1 02:15 all_results.json\n",
      "drwxr-xr-x  2 root root      4096 Aug  1 02:14 \u001b[01;34mcheckpoint-16290\u001b[0m/\n",
      "drwxr-xr-x  2 root root      4096 Aug  1 02:00 \u001b[01;34mcheckpoint-8145\u001b[0m/\n",
      "-rw-r--r--  1 root root      1473 Aug  1 02:14 config.json\n",
      "-rw-r--r--  1 root root       780 Aug  1 02:15 eval_results.json\n",
      "-rw-r--r--  1 root root       538 Aug  1 02:14 gaudi_config.json\n",
      "-rw-r--r--  1 root root       189 Aug  1 02:14 generation_config.json\n",
      "drwxr-x---  2 root root      4096 Aug  1 02:34 \u001b[01;34m.graph_dumps\u001b[0m/\n",
      "-rw-r--r--  1 root root 439464805 Aug  1 02:14 pytorch_model.bin\n",
      "drwxr-xr-x  3 root root      4096 Aug  1 01:45 \u001b[01;34mruns\u001b[0m/\n",
      "-rw-r--r--  1 root root      2201 Aug  1 02:14 special_tokens_map.json\n",
      "-rw-r--r--  1 root root    791656 Aug  1 02:14 spiece.model\n",
      "-rw-r--r--  1 root root      2324 Aug  1 02:14 tokenizer_config.json\n",
      "-rw-r--r--  1 root root   2422358 Aug  1 02:14 tokenizer.json\n",
      "-rw-r--r--  1 root root      8497 Aug  1 02:14 trainer_state.json\n",
      "-rw-r--r--  1 root root      4155 Aug  1 02:14 training_args.bin\n",
      "-rw-r--r--  1 root root       313 Aug  1 02:14 train_results.json\n"
     ]
    }
   ],
   "source": [
    "%ls -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0a66d44-7e1f-4257-9ced-866898fb793c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'T5ForConditionalGeneration' is not supported for question-answering. Supported models are ['AlbertForQuestionAnswering', 'BartForQuestionAnswering', 'BertForQuestionAnswering', 'BigBirdForQuestionAnswering', 'BigBirdPegasusForQuestionAnswering', 'BloomForQuestionAnswering', 'CamembertForQuestionAnswering', 'CanineForQuestionAnswering', 'ConvBertForQuestionAnswering', 'Data2VecTextForQuestionAnswering', 'DebertaForQuestionAnswering', 'DebertaV2ForQuestionAnswering', 'DistilBertForQuestionAnswering', 'ElectraForQuestionAnswering', 'ErnieForQuestionAnswering', 'ErnieMForQuestionAnswering', 'FlaubertForQuestionAnsweringSimple', 'FNetForQuestionAnswering', 'FunnelForQuestionAnswering', 'GPTJForQuestionAnswering', 'IBertForQuestionAnswering', 'LayoutLMv2ForQuestionAnswering', 'LayoutLMv3ForQuestionAnswering', 'LEDForQuestionAnswering', 'LiltForQuestionAnswering', 'LongformerForQuestionAnswering', 'LukeForQuestionAnswering', 'LxmertForQuestionAnswering', 'MarkupLMForQuestionAnswering', 'MBartForQuestionAnswering', 'MegaForQuestionAnswering', 'MegatronBertForQuestionAnswering', 'MobileBertForQuestionAnswering', 'MPNetForQuestionAnswering', 'MvpForQuestionAnswering', 'NezhaForQuestionAnswering', 'NystromformerForQuestionAnswering', 'OPTForQuestionAnswering', 'QDQBertForQuestionAnswering', 'ReformerForQuestionAnswering', 'RemBertForQuestionAnswering', 'RobertaForQuestionAnswering', 'RobertaPreLayerNormForQuestionAnswering', 'RoCBertForQuestionAnswering', 'RoFormerForQuestionAnswering', 'SplinterForQuestionAnswering', 'SqueezeBertForQuestionAnswering', 'XLMForQuestionAnsweringSimple', 'XLMRobertaForQuestionAnswering', 'XLMRobertaXLForQuestionAnswering', 'XLNetForQuestionAnsweringSimple', 'XmodForQuestionAnswering', 'YosoForQuestionAnswering'].\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import habana_frameworks.torch\n",
    "\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load model to fine-tune and its tokenizer\n",
    "model_to_finetune = \"t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_to_finetune)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_to_finetune)\n",
    "\n",
    "# Point to the ft-summarization folder with the fine-tuned model\n",
    "path_to_local_model = \"/root/shared/demo/optimum-habana/examples/seq2seq_squad\"\n",
    "\n",
    "# Instantiate pipeline from local repo, if you did not run the fine tuning step above, you can change: model=model_to_finetune\n",
    "qa = pipeline(task=\"question-answering\", model=path_to_local_model, device=\"hpu\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc46d1ac-be72-4f2c-8ebb-82ff3255af1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please input your question :  What kind of economy does Victoria have?\n",
      "Please input your context:  The economy of Victoria is highly diversified: service sectors including financial and property services, health, education, wholesale, retail, hospitality and manufacturing constitute the majority of employment. Victoria's total gross state product (GSP) is ranked second in Australia, although Victoria is ranked fourth in terms of GSP per capita because of its limited mining activity. Culturally, Melbourne is home to a number of museums, art galleries and theatres and is also described as the \"sporting capital of Australia\". The Melbourne Cricket Ground is the largest stadium in Australia, and the host of the 1956 Summer Olympics and the 2006 Commonwealth Games. The ground is also considered the \"spiritual home\" of Australian cricket and Australian rules football, and hosts the grand final of the Australian Football League (AFL) each year, usually drawing crowds of over 95,000 people. Victoria includes eight public universities, with the oldest, the University of Melbourne, having been founded in 1853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What kind of economy does Victoria have?\n",
      "Answer: 'highly diversified' with score 0.6703124046325684\n"
     ]
    }
   ],
   "source": [
    "# QnA questions\n",
    "\n",
    "question = input (\"Please input your question : \")\n",
    "context = input (\"Please input your context: \")\n",
    "# Generating an answer to the question in context\n",
    "#qa = pipeline(\"question-answering\")\n",
    "answer = qa(question=question, context=context)\n",
    "\n",
    "# Print the answer\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: '{answer['answer']}' with score {answer['score']}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756d203f-3f60-43fc-9fcc-2f555c6ab89a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
